---
title: "aqg_sys_rev"
author: "Ghader"
date: "8 September 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(bib2df)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)

clean <- function(df){
df <- as.data.frame(df)
df <- df  %>% mutate_all(funs(sub("\\.$", "",.)))
# } not working
df <- df  %>% mutate_all(funs(sub("}", "",.)))
df <- df  %>% mutate_all(funs(gsub(",", "",.)))
df <- df  %>% mutate_all(funs(gsub("-", " ",.)))
df <- df  %>% mutate_all(funs(gsub(":", "",.)))
df <- df  %>% mutate_all(funs(gsub("\\s+", " ",.)))
df <- df  %>% mutate_all(funs(tolower))
df <- df  %>% mutate_all(funs(trimws(.,which = "both")))
return(df)
}

remove_duplicates <- function(df){
file_no_duplic <-
  df %>%
  group_by(title) %>%
  summarize(n = n())
return(file_no_duplic)
}

add_ids <- function(df, x){
file_with_id <- merge(df, bib_ref_names, by.x = x, by.y = "TITLE", all.x = TRUE)
return(file_with_id)
}

catch_na_ids <- function(df){
no_id <-
  df %>%
  filter(is.na(BIBTEXKEY))
#write.csv(no_id,'check_this.csv')
return(no_id)
}

# for thoese file that automatic assignment of ids did not work
add_na_id <- function(df){
  # add if the file is snowballing
file_add_id <- df
file_add_id$BIBTEXKEY[file_add_id$title == "outcome based predictive analysis of automatic question paper using data mining"] <- "bindra2017outcome"
file_add_id$BIBTEXKEY[file_add_id$title == "vqa inverse visual question answering"] <- "antol2015vqa"
file_add_id$BIBTEXKEY[file_add_id$title == "web authoriser tool to build assessments using wikipedia articles"] <- "adithya2017web"
file_add_id$BIBTEXKEY[file_add_id$title == "evaluation of a question generation approach using open linked data for supporting argumentation"] <- "Le2015evaluation"
file_add_id$BIBTEXKEY[file_add_id$title == "automatically generate quizzes using text embedding"] <- "asian12017"
file_add_id$BIBTEXKEY[file_add_id$title == "rebial un juego de preguntas generadas a partir de la dbpedia"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$title == "automatic generation of questions from product manual sentences"] <- "asian2018"
file_add_id$BIBTEXKEY[file_add_id$title == "an interactive question raising system for recalling personal photos"] <- "wu2017"
file_add_id$BIBTEXKEY[file_add_id$title == "active learning method through question generation question and answer"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$title == "lod based semantically enhanced open learning space raise engagement for historical deep consideration"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "the prospect of open online e learning system based on the free culture movement development of the yoututors as an auto assignment generator by utilizing creative commons contents online"] <- "nakajima2015prospect"

file_add_id$BIBTEXKEY[file_add_id$relv_title == "outcome based predictive analysis of automatic question paper using data mining"] <- "bindra2017outcome"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "vqa inverse visual question answering"] <- "antol2015vqa"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "web authoriser tool to build assessments using wikipedia articles"] <- "adithya2017web"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "evaluation of a question generation approach using open linked data for supporting argumentation"] <- "Le2015evaluation"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "automatically generate quizzes using text embedding"] <- "asian12017"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "rebial un juego de preguntas generadas a partir de la dbpedia"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "automatic generation of questions from product manual sentences"] <- "asian2018"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "an interactive question raising system for recalling personal photos"] <- "wu2017"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "active learning method through question generation question and answer"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "lod based semantically enhanced open learning space raises engagement for historical deep consideration"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "a novel approach to qenerate mcqs from domain ontology considering dl semantics and open world assumption"] <- "vinu2015novel"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "building an agent for factual question generation task"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "factors of difficulty in german language proficiency tests"] <- "nf"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "online test system to reduce teachers' workload for item and test preparation"] <- "aggrey2017online"
file_add_id$BIBTEXKEY[file_add_id$relv_title == "evaluating human and automated generation of distractors for diagnostic multiple choice cloze questions to assess children's reading comprehension"] <- "huang2015evaluating"


file_add_id$BIBTEXKEY[file_add_id$relv_title == "automatic generation of english reference question by utilising nonerestrictive relative clause"] <- "satria2017automatic"

file_add_id$BIBTEXKEY[file_add_id$relv_title == "generating indonesian question automatically based on bloom's taxonomy using template based method"] <- "kusuma2018generating"

file_add_id$BIBTEXKEY[file_add_id$relv_title == "neural generation of diverse questions using answer focus"] <- "harrison2018neural"

file_add_id$BIBTEXKEY[file_add_id$title == "automatic generation of english reference question by utilising nonerestrictive relative clause"] <- "satria2017automatic"

file_add_id$BIBTEXKEY[file_add_id$title == "the art of deep connection towards natural and pragmatic n interactions"] <- "ray2017art"

return(file_add_id)
}

in_incl <- function(file_with_id){
common_inc <- as.data.frame(intersect(file_with_id$BIBTEXKEY, included_no_dupl$id), stringsAsFactors=FALSE)
return(common_inc)
}

in_excl <- function(file_with_id){
common_inc <- as.data.frame(intersect(file_with_id$BIBTEXKEY, excluded$id), stringsAsFactors=FALSE)
return(common_inc)
}

not_in_all <-  function(file_with_id){
dif_inc <- as.data.frame(setdiff(file_with_id$BIBTEXKEY,all$id), stringsAsFactors=FALSE) 
return(dif_inc)
}
  
not_in_incl <- function(file_with_id){
dif_inc <- as.data.frame(setdiff(file_with_id$BIBTEXKEY,included_no_dupl$id), stringsAsFactors=FALSE) 
return(dif_inc)
}

not_in_excl <- function(file_with_id){
dif_inc <- as.data.frame(setdiff(file_with_id$BIBTEXKEY,excluded$id), stringsAsFactors=FALSE) 
return(dif_inc)
}

#-----------------------

count_question_format <- function(review){
x <- 
  review %>%
  group_by(question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_input_question_format <- function(review){
x <- 
  review %>%
  group_by(input, question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_method_question_format <- function(review){
x <- 
  review %>%
  group_by(method, question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_response_format <- function(review){
x <- 
review %>%
  group_by(response_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_input <- function(review){
x <- 
review %>%
  group_by(input) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_input2 <- function(review){
x <- 
review %>%
  group_by(input2) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

add_percent <- function(df){
df <- df %>% mutate(percentage = (df$n / sum(df$n) * 100))
df <- df %>% mutate(string_per = paste(as.character(round(percentage, 1)),'%',sep=''))
#df <- df %>% mutate(string_n = paste(' (',as.character(n),')',sep=''))
df <- df %>% mutate(string_n = as.character(n))
return(df)
}
count_input_domain <- function(review){
# cannot see relation between input and domain
x <- 
review %>%
  group_by(input, domain) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)

}

count_input_response_format <- function(review){
x <- 
review %>%
  group_by(input,response_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)

}


count_input_response_format_domain <- function(review){
x <- 
  review %>%
  group_by(input,response_format,domain) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)

}

count_input_response_format_domain_input2 <- function(review){
x <- 
  review %>%
  group_by(input,response_format,domain,input2) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)

}

count_input_response_format_input2 <- function(review){
x <- 
  review %>%
  group_by(input,input2,response_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)

}

count_method <- function(review){
x <- 
review %>%
  group_by(method) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_method2 <- function(review){
x <- 
review %>%
  group_by(method2) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_method_input <- function(review){
x <- 
review %>%
  group_by(method, input) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_purpose <- function(review){
x <- 
review %>%
  group_by(purpose) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_domain <- function(review){
x <- 
review %>%
  group_by(domain) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_evaluation <- function(review){
x <-
  review %>%
  group_by(evaluation) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
return(x)
}

count_year <- function(review){
x <-
  review %>%
  group_by(YEAR) %>%
  summarize(n = n()) %>%
  arrange(desc(YEAR))
return(x)
}

```

# https://cran.r-project.org/web/packages/bib2df/vignettes/bib2df.html

```{r read_files}
dir <- "C:/Users/ghade/Dropbox/aqg_sys_rev/r/"
#dir <- "C:/Users/kurdig/Dropbox/aqg_sys_rev/r/"

bib_ref_f <- paste(dir,"ref.bib",sep="")
bib_ref <- bib2df(bib_ref_f, separate_names = TRUE)
bib_ref_names <- bib_ref %>% dplyr::select(BIBTEXKEY, TITLE)
bib_ref_names <- clean(bib_ref_names)
bib_ref_auth <- bib2df(bib_ref_f, separate_names = FALSE)


thani_bib_ref_f <- paste(dir,"thani_ref.bib",sep="")
thani_bib_ref <- bib2df(thani_bib_ref_f, separate_names = TRUE)

all_f <- paste(dir,"all.csv",sep="")
all <- read.csv(all_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
all <- clean(all)

included <- all %>% filter(all$included == "yes")
included_no_dupl <- included %>% distinct(id, .keep_all = TRUE)
#included_no_dupl <- clean(included_no_dupl)

included_full_info <- merge(included, bib_ref, by.x = "id", by.y = "BIBTEXKEY")
included_full_info$YEAR[included_full_info$id == "adithya2017web"] <- 2017
included_full_info <- clean(included_full_info)

# search result
ieee_f <- paste(dir,"ieee.csv",sep="")
sc_f <- paste(dir,"ScienceDirect.csv",sep="")
inspec_f <- paste(dir,"inspec.csv",sep="")
eric_f <- paste(dir,"eric.csv",sep="")

ieee <- read.csv(ieee_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
ieee <- ieee %>% select(title)
ieee$source <- "ieee"
sc <- read.csv(sc_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
sc$source <- "sc"
inspec <- read.csv(inspec_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
inspec$source <- "inspec"
eric <- read.csv(eric_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
eric$source <- "eric"
main_all <- rbind(ieee,sc,inspec,eric)
main_all <- clean(main_all)
colnames(main_all) <- c("title","V2")
# included based on abstract
main_all_no_dupl <- remove_duplicates(main_all)
main_all_with_id <- add_ids(main_all_no_dupl, "title")

ieee_inc_f <- paste(dir,"ieee_incl.csv",sep="")
sc_inc_f <- paste(dir,"ScienceDirect_incl.csv",sep="")
inspec_inc_f <- paste(dir,"inspec_incl.csv",sep="")
acm_inc_f <- paste(dir,"acm_incl.csv",sep="")
eric_inc_f <- paste(dir,"eric_incl.csv",sep="")
aied_inc_f <- paste(dir,"aied_incl.csv",sep="")

ieee_inc <- read.csv(ieee_inc_f, header=FALSE, sep=",", fileEncoding="UTF-8-BOM")
nrow(ieee_inc)
sc_inc <- read.csv(sc_inc_f, header=FALSE, sep=",", fileEncoding="UTF-8-BOM")
nrow(sc_inc)
inspec_inc <- read.csv(inspec_inc_f, header=FALSE, sep=",", fileEncoding="UTF-8-BOM")
nrow(inspec_inc)
acm_inc <- read.csv(acm_inc_f, header=FALSE, sep=",", fileEncoding="UTF-8-BOM")
nrow(acm_inc)
eric_inc <- read.csv(eric_inc_f, header=FALSE, sep=",", fileEncoding="UTF-8-BOM")
nrow(eric_inc)
aied_inc <- read.csv(aied_inc_f, header=FALSE, sep=",", fileEncoding="UTF-8-BOM")
nrow(aied_inc)

nrow(ieee_inc) + nrow(sc_inc) + nrow(inspec_inc) + nrow(acm_inc) + nrow(eric_inc) + nrow(aied_inc)

main_included_abstract <- rbind(ieee_inc,sc_inc,inspec_inc,acm_inc,eric_inc,aied_inc)
colnames(main_included_abstract) <- c("title","V2")
# included based on abstract
main_included_abstract <- remove_duplicates(main_included_abstract)
nrow(main_included_abstract)

# old verson: new_sys_f <- paste(dir,"included_data_extraction_clean.csv",sep="")
new_sys_f <- paste(dir,"data_extraction (this one).csv",sep="")
#new_sys_f <- paste(dir,"data_extraction_this.csv",sep="")
new_sys <- read.csv(new_sys_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
new_sys <- clean(new_sys)

new_sys_full_info <- merge(new_sys, bib_ref, by.x = "id", by.y = "BIBTEXKEY")
new_sys_full_info <- new_sys_full_info %>% select(id,CATEGORY)

old_sys_f <- paste(dir,"thani_sys.csv",sep="")
old_sys <- read.csv(old_sys_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM") 
old_sys <- old_sys %>% filter(source != "Redundant")
old_sys <- old_sys  %>% mutate_all(funs(sub(",", ";",.)))

excluded <- all %>% filter(all$included == "no")
excluded <- clean(excluded)
reasons_for_excl <- excluded %>% group_by(reason) %>% summarize(n = n())

maybe_include <- excluded %>% filter(reason == "no sufficient description of qg")

ns <- all %>% filter(all$included == "uni")

snowballing_f <- paste(dir,"snowballing.csv",sep="")
snowballing <- read.csv(snowballing_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
snowballing <- clean(snowballing)
snowballing_no_duplic <- remove_duplicates(snowballing)
snowballing_with_id <-add_ids(snowballing_no_duplic, "title")
snowballing_with_id <-add_na_id(snowballing_with_id)
# quality check
snowballing_no_id <- catch_na_ids(snowballing_with_id)

google_f <- paste(dir,"google citation.csv",sep="")
google <- read.csv(google_f, header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
google <- clean(google)
google_no_duplic <- remove_duplicates(google)
google_with_id <-add_ids(google_no_duplic, "title")
# quality check
google_no_id <- catch_na_ids(google_with_id)

```

```{r quality_check}
common <- as.data.frame(intersect(new_sys$id, included_no_dupl$id), stringsAsFactors=FALSE)  

new_sys_id <- separate_rows(new_sys,id,sep=";\\s+")

# not in included but in extracted
dif <- as.data.frame(setdiff(new_sys_id$id, included_no_dupl$id), stringsAsFactors=FALSE) 

# not in extracted but in included
dif2 <- as.data.frame(setdiff(included_no_dupl$id,new_sys_id$id), stringsAsFactors=FALSE)

# Is there any excluded in data extraction
x <- as.data.frame(intersect(new_sys$id, excluded$id), stringsAsFactors=FALSE)

```

## included based on abstract 

```{r abstract}

nrow(ieee_inc)
nrow(sc_inc)
nrow(inspec_inc)
nrow(acm_inc)
nrow(eric_inc)
nrow(aied_inc)

nrow(ieee_inc) + nrow(sc_inc) + nrow(inspec_inc) + nrow(acm_inc) + nrow(eric_inc) + nrow(aied_inc)
```

## included based on full text 
```{r full text}

common_eric_incl <- as.data.frame(intersect(eric_inc$V1, included$id), stringsAsFactors=FALSE)  
nrow(common_eric_incl)

common_acm_incl <- as.data.frame(intersect(acm_inc$V1, included$id), stringsAsFactors=FALSE)  
nrow(common_acm_incl)

common_inspec_incl <- as.data.frame(intersect(inspec_inc$V1, included$id), stringsAsFactors=FALSE)  
nrow(common_inspec_incl)

common_ieee_incl <- as.data.frame(intersect(ieee_inc$V1, included$id), stringsAsFactors=FALSE)  
nrow(common_ieee_incl)

common_aied_incl <- as.data.frame(intersect(aied_inc$V1, included$id), stringsAsFactors=FALSE)  
nrow(common_aied_incl)

nrow(common_eric_incl) + nrow(common_acm_incl) + nrow(common_inspec_incl) + nrow(common_ieee_incl) + nrow(common_aied_incl) + nrow(sc_inc)

#How many of main were really included (after reding full text)
common_intial_incl <- as.data.frame(intersect(main_included_abstract$title, included_no_dupl$id), stringsAsFactors=FALSE)  
colnames(common_intial_incl) <- c("id")
nrow(common_intial_incl)
```

## Snowballing
```{r snowballing}

# check that all snowbaliing are in all
snowballing_dif_all<- not_in_all(snowballing_with_id)

#check that you did snowballing for all results
all_dif_snowballing <- as.data.frame(setdiff(all$id,snowballing_with_id$BIBTEXKEY), stringsAsFactors=FALSE) 

snowballing_common_incl <- in_incl(snowballing_with_id)
snowballing_common_excl <- in_excl(snowballing_with_id)

# how many from snowballing and how many from google
snowball_result <-
  snowballing %>%
  group_by(relv_title) %>%
  summarize(n = n())

snowball_result <- snowball_result  %>% filter(relv_title != "not related", relv_title != "none (no full text avaliable)", relv_title != "none", relv_title != "no full text")

nrow(snowball_result)

snowball_result_with_id <- add_ids(snowball_result, "relv_title")
snowball_result_with_id <- add_na_id(snowball_result_with_id) 
snowball_result_with_id <- snowball_result_with_id %>% filter (!is.na(BIBTEXKEY))

# you need to check this
dif <- as.data.frame(setdiff(snowball_result$relv_title,snowball_result_with_id$relv_title), stringsAsFactors=FALSE) 



no_id <-
 snowball_result_with_id %>%
 filter(is.na(BIBTEXKEY))
nrow(no_id)

# How many from snowball result are in included and excluded
# have you done inclusion exclusion for all snowball
snowballing_result_common_incl <- in_incl(snowball_result_with_id)
nrow(snowballing_result_common_incl)
snowballing_result_common_excl <- in_excl(snowball_result_with_id)
nrow(snowballing_result_common_excl)

snowballing_result_dif_all <- not_in_all(snowball_result_with_id)
nrow(snowballing_result_dif_all)
```


```{r google}

#check that you did google for all included and excluded
google_common_incl <- in_incl(google_with_id)
nrow(google_common_incl)
google_common_excl <- in_excl(google_with_id)
google_dif_all <- not_in_all(google_with_id)
all_dif_google <- as.data.frame(setdiff(all$id,google_with_id$BIBTEXKEY), stringsAsFactors=FALSE) 

# How many google with no snowball and via versa
# I dont need this if thery are in all
snowballing_dif_google <- dif_inc <- as.data.frame(setdiff(snowballing_with_id$BIBTEXKEY, google_with_id$BIBTEXKEY), stringsAsFactors=FALSE) 
google_dif_snowballing <- dif_inc <- as.data.frame(setdiff(google_with_id$BIBTEXKEY, snowballing_with_id$BIBTEXKEY), stringsAsFactors=FALSE) 

#----------------------------------------------

# how many from snowballing and how many from google
google_result <-
  google %>%
  group_by(relv_title) %>%
  summarize(n = n())

nrow(google_result)

google_result_with_id <- add_ids(google_result, "relv_title")
google_result_with_id <- add_na_id(google_result_with_id) 
google_result_with_id <- google_result_with_id %>% filter (!is.na(BIBTEXKEY))
no_id <-
 google_result_with_id %>%
 filter(is.na(BIBTEXKEY))

# sum(google1$cited_by)
# sum(google1$relv)

# have you done inclusion exclusion for all google
google_result_dif_all <- not_in_all(google_result_with_id)
nrow(google_result_dif_all)

# How many from google result are in included and excluded

google_result_common_incl <- in_incl(google_result_with_id)
nrow(google_result_common_incl)
google_result_common_excl <- in_excl(google_result_with_id)
nrow(google_result_common_excl)
```

# Overlap between google and snowball results
# How many captured in the intial result and how many new

```{r sg_combined}

common_sg <-  rbind(snowballing_result_common_incl,google_result_common_incl)
colnames(common_sg) <- c("title")
common_sg <- common_sg %>% filter(!is.na(id),id != "nf")
# number of duplicates
nrow(common_sg)
common_sg <- remove_duplicates(common_sg)
nrow(common_sg)

common_sg <- as.data.frame(intersect(google_result_with_id$BIBTEXKEY, snowball_result_with_id$BIBTEXKEY), stringsAsFactors=FALSE)
colnames(common_sg) <- c("id")
common_sg <- common_sg %>% filter(!is.na(id),id != "nf")
# number of duplicates
nrow(common_sg)

common_sg <- rbind(google_result_with_id,snowball_result_with_id)
# before duplicates
nrow(common_sg)
colnames(common_sg) <- c("title", "n", "BIBTEXKEY")
common_sg <- remove_duplicates(common_sg)
common_sg <- add_ids(common_sg, "title")
common_sg <- add_na_id(common_sg) 
# after duplicates
nrow(common_sg)

# overlap between snowball-google and included based on abstract
common_sg_initial <- as.data.frame(intersect(common_sg$BIBTEXKEY, main_included_abstract$title), stringsAsFactors=FALSE)
nrow(common_sg_initial)

# overlap between snowball and included based on full text
common_intial_sg <- as.data.frame(intersect(common_intial_incl$`intersect(main_included_abstract$title, included_no_dupl$id)`, common_sg$BIBTEXKEY), stringsAsFactors=FALSE)  
nrow(common_intial_sg)
```


## comparsion between Thani results and my results
### What type of questions

```{r analysis}
qf_old <- count_question_format(old_sys)
qf_old$source <- "Thani"
qf_old <- separate_rows(qf_old,question_format,sep=";\\s+")


qf_new <- count_question_format(new_sys)
qf_new$source <- "Ghader"
qf_new <- separate_rows(qf_new,question_format,sep=";\\s+")

qf_both <- rbind(qf_old, qf_new)

x <- qf_both %>% group_by(source,question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

# Top Thani
t <- qf_old %>% group_by(source,question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
t

# Top Ghader
g <- qf_new %>% group_by(source,question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
g

# overlap

# new type of questions 

# discontinued question 


# ggplot(qf_both, aes(question_format, n))+
#  geom_bar(stat="identity") +
#  facet_grid(source ~.) +
#  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

# is some input more appropriate for some questions
iqf_old <-count_input_question_format(old_sys)
iqf_old$source <- "thani"
iqf_new <-count_input_question_format(new_sys)
iqf_new$source <- "me"
iqf_both <- rbind(iqf_old, iqf_new)
iqf_both <- separate_rows(iqf_both,question_format,sep=";\\s+")

x <- iqf_both %>% group_by(source,input,question_format) %>%
  summarize(n = n())
x

# ggplot(iqf_both, aes(question_format, n, fill=input))+
#  geom_bar(stat="identity") +
#  facet_grid(source ~.) +
#  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))
```

### what type of methods
```{r method}

mqf_old <-count_method_question_format(old_sys)
mqf_old$source <- "thani"
mqf_old <- separate_rows(mqf_old,method,sep=";\\s+")

mqf_new <-count_method_question_format(new_sys)
mqf_new$source <- "me"
mqf_new <- separate_rows(mqf_new,method,sep=";\\s+")

mqf_both <- rbind(mqf_old, mqf_new)
qf_both <- separate_rows(qf_both,question_format,sep=";\\s+")

# Top Thani
t <- mqf_old %>% group_by(method) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
t

# Top Ghader
g <- mqf_new %>% group_by(method) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
g

ggplot(mqf_both, aes(question_format, n, fill=method))+
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))
```

```{r response format}

rf_old <-count_response_format(old_sys)
rf_old$source <- "thani"
rf_new <-count_response_format(new_sys)
rf_new$source <- "me"
rf_both <- rbind(rf_old, rf_new)
rf_both <- separate_rows(rf_both,response_format,sep=";\\s+")

ggplot(rf_both, aes(response_format, n))+
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))
```

```{r input}

i_old <-count_input(old_sys)
i_old$source <- "thani"

i_new <-count_input(new_sys)
i_new$source <- "me"

i_new <-count_input2(new_sys)
i_new$source <- "me"

i_new$input[i_new$input=="question stem; question key; a distractor set to rank (in ranking mc)"] <- "stem, key, and a distractor set"
i_new$input[i_new$input=="target word with its part of speech and a word sense; wordnet"] <- "target word with its POS and WS; WordNet"
i_new$input[i_new$input=="text; question key"] <- "text and key"
i_new$input[i_new$input=="question stem; question key"] <- "stem and key"

i_new <- add_percent(i_new)
i_old <- add_percent(i_old)
i_both <- rbind(i_old, i_new)
  
ggplot(i_both, aes(input, n))+
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
 # geom_text( aes(label=string_per), vjust=-.25, hjust = 1, size=3) +
 # geom_text( aes(label=string_n), vjust=-.25, hjust = 0, size=3) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

ggplot(i_both, aes(input, n, fill=source))+
  geom_bar(stat="identity", position = "dodge") +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

#-------------------------------------------------------------------------------------
#input and domain

id_old <-count_input_domain(old_sys)
id_old$source <- "thani"

id_new <-count_input_domain(new_sys)
id_new$source <- "me"

id_both <- rbind(id_old, id_new)

ggplot(id_both, aes(domain, n, fill=input))+
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

irf_old <-count_input_response_format(old_sys)
irf_old$source <- "thani"

irf_new <-count_input_response_format(new_sys)
irf_new$source <- "me"

irf_both <- rbind(irf_old, irf_new)
ggplot(irf_both, aes(response_format, n, fill=input))+
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

#-------------------------------------------------------------------------------------

#input and response format()
# rf_both <- separate_rows(rf_both,response_format,sep=";\\s+")

irfd_old <-count_input_response_format_domain(old_sys)
irfd_old$source <- "thani"
irfd_new <-count_input_response_format_domain(new_sys)
irfd_new$source <- "me"
irfd_both <- rbind(irfd_old,irfd_new)
#-------------------------------------------------------------------------------------

# what additional sources are required for genrating generic question from text
# what additional sources are required for genrating mcq from text

irfdi_old <-count_input_response_format_domain_input2(old_sys)
irfdi_old$source <- "thani"

irfdi_new <-count_input_response_format_domain_input2(new_sys)
irfdi_new$source <- "me"

irfdi_both <- rbind(irfdi_old, irfdi_new)

# input domain response format question type
# what type of questions are generic mcqs from text

x <-  new_sys %>%
  group_by(input,response_format,domain,question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))


#-------------------------------------------------------------------------------------

irfi_old <-count_input_response_format_input2(old_sys)
irfi_old$source <- "thani"
irfi_new <-count_input_response_format_input2(new_sys)
irfi_new$source <- "me"
irfi_both <- rbind(irfi_old, irfi_new)

#-------------------------------------------------------------------------------------
```


```{r method}

m_old <-count_method(old_sys)
m_old$source <- "thani"
m_new <-count_method(new_sys)
m_new$source <- "me"
m_both <- rbind(m_old, m_new)
m_new <- separate_rows(m_new,method,sep=";\\s+")


m_old <-count_method2(old_sys)
m_old$source <- "thani"
m_new <-count_method2(new_sys)
m_new$source <- "me"
m_both <- rbind(m_old, m_new)
m_new <- separate_rows(m_new,method2,sep=";\\s+")


ggplot(m_both, aes(method, n))+
geom_bar(stat="identity") +
facet_grid(source ~.) +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

mi_old <-count_method_input(old_sys)
mi_old$source <- "thani"
mi_new <-count_method_input(new_sys)
mi_new$source <- "me"
mi_both <- rbind(mi_old, mi_new)
ggplot(mi_both, aes(method, n, fill=input))+
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

p_old <-count_purpose(old_sys)
p_old$source <- "thani"
new_sys_purpose <- separate_rows(new_sys,purpose,sep=";\\s+")
p_new <-count_purpose(new_sys_purpose)

p_new$source <- "me"

p_both <- rbind(p_old, p_new)
ggplot(p_both, aes(purpose, n))+
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

e_old <-count_evaluation(old_sys)
e_old$source <- "thani"

new_sys_eval <- separate_rows(new_sys,evaluation,sep=";\\s+")
e_new <-count_evaluation(new_sys_eval)
e_new$source <- "me"

e_both <- rbind(e_old, e_new)
ggplot(e_both, aes(evaluation, n)) +
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))

```

```{r domain}

d_old <-count_domain(old_sys)
d_old$source <- "thani"

d_new <-count_domain(new_sys)
d_new$source <- "me"

d_both <- rbind(d_old, d_new)
ggplot(d_both, aes(domain, n)) +
  geom_bar(stat="identity") +
  facet_grid(source ~.) +
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=.5,colour='gray50'))


x <-  new_sys %>%
  group_by(domain,question_format) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
```

## Types of papers and Publication venues

```{r obj1}
x <- included_full_info %>% select(id,X.NOTE)
# What type of research
included_full_info %>%
  group_by(CATEGORY) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

# What type of research in Thani review
thani_bib_ref %>%
  group_by(CATEGORY) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

conf <- thani_bib_ref %>%
  filter(CATEGORY=="INPROCEEDINGS") %>% select(BOOKTITLE)
conf <- clean(conf)


library(stringr)

workshop <- conf %>% filter(str_detect(BOOKTITLE, "workshop"))
symo <- conf %>% filter(str_detect(BOOKTITLE, "symposium"))

nrow(conf) - nrow(workshop) - nrow(symo)


# where reseacrh is published (journals)?
journals <-
included_full_info %>%
  filter(!is.na(JOURNAL)) %>%
  group_by(JOURNAL) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  slice(1:5)

top_journal <-
  thani_bib_ref %>%
  filter(!is.na(JOURNAL)) %>%
  group_by(JOURNAL) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) 

x <- clean(thani_bib_ref) %>% select(JOURNAL)
y <- clean(included_full_info) %>% select(JOURNAL)
top_journal <- rbind(x,y) %>%
  filter(!is.na(JOURNAL)) %>%
  group_by(JOURNAL) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) 


# where reseacrh is published (confernces)?

require("tm")

words = c("2nd","3rd","4th","5th","6th","7th","8th","9th","11th","12th","13th", "14th" , "17th","22nd", "25th","26th","27th","28th","36th","49th", '52nd', "fourth","fifth","seventh","eighth i", "tenth","thirteenth","twenty","2009","2010", "2011", "2012", "2013", "2014" ,"2015","2016","2017","2018", "the", "proceedings of")
included_full_info$BOOKTITLE <- removeWords(included_full_info$BOOKTITLE,words)
thani_bib_ref$BOOKTITLE <- removeWords(thani_bib_ref$BOOKTITLE,words)

x <- clean(thani_bib_ref) %>% select(BOOKTITLE)
y <- clean(included_full_info) %>% select(BOOKTITLE)
top_conf <- rbind(x,y) %>%
  filter(!is.na(BOOKTITLE)) %>%
  group_by(BOOKTITLE) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

conf <-
  included_full_info %>%
  filter(!is.na(BOOKTITLE)) %>%
  group_by(BOOKTITLE) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  slice(1:5)

top_conf <-
  thani_bib_ref %>%
  filter(!is.na(BOOKTITLE)) %>%
  group_by(BOOKTITLE) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) 

 



included_full_info %>% 
  select(YEAR, id) %>% 
  unnest() %>% 
  ggplot() + 
  aes(x = YEAR, y = reorder(id, desc(YEAR))) + 
  geom_point()

included_full_info %>%
  group_by(YEAR) %>%
  summarize(n = n()) %>%
  arrange(desc(YEAR))
```

## Rate of publications

```{r Rate of publications}

included_year <- included_full_info %>% dplyr::select(id,YEAR)  %>% filter(!is.na(YEAR))
#included_year$source <- "This review"
old_sys_year <- old_sys %>% dplyr::select(id,YEAR)
#old_sys_year$source <- "Alsubait's review"

old_missing_years <- data.frame("YEAR" = c(2001,2000,1998,1996,1995,1994,1993,1992,1990,1989,1988,1986,1985,1984,1982,1981,1980,1979,1978,1977,1974,1973,1972,1971), "n" = rep(c(0), each=24))

year <- rbind(included_year, old_sys_year)
  
year <- year %>%
  group_by(YEAR) %>%
  summarize(n = n())

year <- rbind(year, old_missing_years)
year <- add_percent(year)
year$string_n[year$string_n == "0"] <- ""

ggplot(year, aes(YEAR, n, group = 1)) +
  geom_line() +
  labs(x = "Year", y = "No. of publications") +
  geom_text( aes(label=string_n), vjust=-.5, hjust = .5, size=3) +
  scale_x_discrete(labels=c("2018" = "18", "2018" = "18", "2017" = "17", "2016" = "16", "2015" = "15", "2014" = "14", "2013" = "13", "2012" = "12", "2011" = "11", "2010" = "10", "2009" = "09", "2008" = "08", "2007" = "07", "2006" = "06", "2005" = "05", "2004" = "04", "2003" = "03", "2002" = "02", "2001" = "01", "2000" = "00", "1999" = "99", "1998" = "98", "1997" = "97", "1996" = "96", "1995" = "95", "1994" = "94", "1993" = "93", "1992" = "92", "1991" = "91", "1990" = "90", "1989" = "89", "1988" = "88", "1987" = "87", "1986" = "86", "1985" = "85", "1984" = "84","1983" = "83", "1982" = "82", "1981" = "81", "1980" = "80", "1979" = "79", "1978" = "78", "1977" = "77", "1976" = "76", "1975" = "75", "1974" = "74", "1973" = "73", "1972" = "72", "1971" = "71", "1970" = "70"))


ggplot(year, aes(YEAR, n)) +
  geom_bar(stat="identity") +
  geom_text( aes(label=string_n), vjust=-.5, hjust = .5, size=3) +
  labs(x = "Year", y = "No. of publications") +
  scale_x_discrete(labels=c("2018" = "18", "2018" = "18", "2017" = "17", "2016" = "16", "2015" = "15", "2014" = "14", "2013" = "13", "2012" = "12", "2011" = "11", "2010" = "10", "2009" = "09", "2008" = "08", "2007" = "07", "2006" = "06", "2005" = "05", "2004" = "04", "2003" = "03", "2002" = "02", "2001" = "01", "2000" = "00", "1999" = "99", "1998" = "98", "1997" = "97", "1996" = "96", "1995" = "95", "1994" = "94", "1993" = "93", "1992" = "92", "1991" = "91", "1990" = "90", "1989" = "89", "1988" = "88", "1987" = "87", "1986" = "86", "1985" = "85", "1984" = "84","1983" = "83", "1982" = "82", "1981" = "81", "1980" = "80", "1979" = "79", "1978" = "78", "1977" = "77", "1976" = "76", "1975" = "75", "1974" = "74", "1973" = "73", "1972" = "72", "1971" = "71", "1970" = "70"))


```

## Research groups

```{r Research groups}

authors <- read.csv(file="C:/Users/ghade/Dropbox/aqg_sys_rev/r/authors.csv", header=TRUE, sep=",", fileEncoding="UTF-8-BOM")
authors_thani <- read.csv(file="C:/Users/ghade/Dropbox/aqg_sys_rev/r/authors_thani.csv", header=TRUE, sep=",", fileEncoding="UTF-8-BOM")

authors <- clean(authors)
authors_thani <- clean(authors_thani)

# not in authors but in included
dif <- as.data.frame(setdiff(included_no_dupl$id,authors$paper), stringsAsFactors=FALSE)

# in authors but not in included
dif <- as.data.frame(setdiff(authors$paper,included_no_dupl$id), stringsAsFactors=FALSE)

auth <-
  authors %>%
  group_by(authors) %>%
  summarize(n = n())
nrow(auth)

t_auth <-
  authors_thani %>%
  group_by(authors) %>%
  summarize(n = n())
nrow(t_auth)


common_auth <- as.data.frame(intersect(t_auth$authors, auth$authors), stringsAsFactors=FALSE)
nrow(common_auth)

authores_all <- rbind(auth,t_auth)
nrow(authores_all)

authores_all <-
  authores_all %>%
  group_by(authors) %>%
  summarize(n = n())
nrow(authores_all)

authores_all <- authores_all[order(as.character(authores_all$authors)),]


```

## interrater agreement on inclusion exclusion
```{r incl_excl}

ieee_salam <- read.csv(file="C:/Users/ghade/Dropbox/aqg_sys_rev/r/ieee_salam.csv", header=TRUE, sep=",", fileEncoding="UTF-8-BOM")

ieee_salam_incl <- ieee_salam %>% filter(ieee_salam$Relevant == "yes")
ieee_salam_incl <- clean(ieee_salam_incl)
ieee_salam_incl_full <- add_ids(ieee_salam_incl, "Title")

common_inc <- as.data.frame(intersect(ieee_salam_incl_full$BIBTEXKEY, included_no_dupl$id), stringsAsFactors=FALSE)

#in IEEE included but not in salam

#In salam but not in included
dif_inc <- as.data.frame(setdiff(ieee_salam_incl_full$BIBTEXKEY,included_no_dupl$id), stringsAsFactors=FALSE) 

```


```{r performance}
# filter included by domain
# filter evaluatin by included in adomain
# compare
```

```{r incl_excl}
included_title <- included_full_info %>% select(id,TITLE)
write.csv(included_title,'included_title.csv')
```


  